<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
  <meta http-equiv="CONTENT-TYPE" content="text/html; charset=us-ascii">

  <title>Readme for normalization reference spectrum experiments</title>
  <style type="text/css">
    .ra {background-color: #FCF6CF; }
    td.id {font-family: monospace; }
    td {vertical-align:text-top; }
  </style>
</head>

<body dir="ltr" lang="en-US">
  <h1>Readme for normalization reference spectrum experiments</h1>

  <p>This directory contains my experiments, results and write-ups on
  the effects of making different choices in creating the reference
  spectra for normalization methods. Primarily these are focused on
  Probabilistic Quotient Normalization (PQN), but Histogram Normalization
  (HN) also uses a reference spectrum.</p>

  <h2>Experiment 1: using the controls or all spectra to generate the
  reference spectrum for PQN</h2>

  <p>This experiment is mainly a proof of concept, illustrating that
  using control spectra to generate the reference spectra can produce
  spurious differences between normalized control and treatment groups
  due to small sample-size effects.</p>

  <h3>Methods</h3>

  <p>In this experiment, random, synthetic data was generated for
  90,000 trials testing 9 different experimental conditions for a
  total of 10,000 spectra per condition. Each trial generated 10
  spectra from the same distribution. The 9 conditions were the number
  of spectra called "control" and the number called "treatment."  In
  the first condition, the first spectrum is called "control" and the
  rest are called "treatment." In the second, the first two spectra
  are "control". And so forth.</p>

  <p>In each trial, normalized spectra are generated by several
  different methods:
    <ol>
      <li>No normalization - the spectra are created already
      normalized, so this corresponds to the ground-truth.</li>
      <li>Sum normalization - the spectra are normalized to have a sum
      of 1000.</li>
      <li>PQN with control reference - the control group is used to
      generate a reference spectrum for PQN after Sum-normalization to
      1000.</li>
      <li>PQN with all reference - the all spectra are used together to 
      generate a reference spectrum for PQN after Sum-normalization to
      1000.</li>
    </ol>
  </p>
  <p>Then, for each normalization method, the spectra are projected
  onto their first two principal components (reflecting the normal
  analysis method). These coordinates are then scaled and translated
  to the unit square (subtract the minimum of each coordinate and
  divide by the maximum). The scaling takes care of the fact that
  normalization is unique only up to a constant scaling factor. (That
  is, if a and b are spectra and x and y are correct normalization
  constants, then cx and cy for all positive c are also correct
  normalization constants.) The translation is not strictly necessary
  but it was easier to do the scaling with it and it does not make a
  difference to distance computations. Finally and the distances
  between the centroids are of the control and treatment groups are
  computed.
  </p>

  <h3>Analysis</h3>
  <h4>Experimental Question</h4>
  <p>The question I want to answer is whether the distance between the
    normalized "control" and "treatment" groups (measured as the
    distance between their centroids) increases over the distance for
    the unnormalized (gold-standard) data. </p>

  <h4>Choosing a test</h4>

  <p>Because squaring and taking the square root likely destroy the
  Gaussian assumption, I decided to use non-parametric tests. Of the
  tests I know, the Wilcoxon Signed Rank test seemed the most
  appropriate. The assumptions
  (<a href="http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test">as
  given in Wikipedia's entry</a>) are:
    <ol>
      <li>Data is paired and comes from the same population.</li>
      <li>Each pair is chosen randomly and independently.</li>
      <li>The data is measured on an interval scale.</li>
    </ol>
  </p>

  <p>All of these assumptions are met by my data. 
    <ol>
      <li>This comes from construction. The different distances are
      different functional products of the same underlying random
      sample. They are clearly dependent on a common factor and that
      factor is generated from one population.</li>
      <li>The underlying spectra are generated pseudo-randomly and
      independently.</li>
      <li>Distances are interval scale.</li>
    </ol>  
  </p>
  

  <p>Matlab only has facilities for calculating the two-sided
  test. Since I did not want to code the test myself, I decided to
  modify the hypothesis to match the hypothesis tested by the
  test. Thus, the revised null-hypothesis is: for a given
  normalization method and experimental condition, the median
  difference between the distance from of that method and the true
  distance is 0. What I'd really like to have tested is the
  null-hypothesis that the median is less than or equal to 0 versus
  the alternative of being positive. But that would have required
  writing my own code for the test.</p>

  <p>Note that I did not do what most texts mention when talking about
  a paired signed rank test, which is testing whether the medians are
  different. To do that, I would have had to make the additional
  assumption that aside from a possible translation, the distributions
  of the true and normalized distances are the same. I do not feel
  justified in making this assumption and it is not necessary for my
  experiment.
  </p>

  <h4>Choosing significance level</h4>

  <p>For each experimental condition I performed 3 tests of the median
  distance-differences. I'd like to limit the error on all three
  simultaneously. So, I need a multiple test correction. Since I
  believe these three questions to be independent, I used the
  &Scaron;id&aacute;k correction to control for multiple testing and
  give a family-wise Type-I error rate of &alpha;=5% for each
  experimental condition. This led me to use a nominal &alpha; of
  1.70% , which is slightly higher than the Bonferroni correction
  &alpha; (for dependent measurements) of 1.67%. I decided to leave
  each experimental condition as it's own test with a 5% chance of
  having a wrong result, so I did not do a correction for the 9
  conditions. If I had, the required &alpha; would be 0.19%
  (0.00189795 if they are independent and 0.0018836 if dependent)</p>
  
  <p>I was torn about not doing the full correction for 27
  simultaneous tests, but I don't want to increase my Type II error
  too much.</p>

  <h3>Results</h3>

  I'll have to wait until I generate them to write this section.

</body>
</html>
