%% LyX 2.0.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage{amsmath}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\newcommand{\owl}[1]{\textit{#1}} % Format owl properties
\newcommand{\func}[1]{\texttt{#1}} % Format function names

\makeatother

\usepackage{babel}
\setcounter{secnumdepth}{5}
\begin{document}

\title{Proposal for Removing Twiddle Parameters in LOD Property Alignment
Algorithm}


\author{Eric Moyer}


\date{28 August 2012}

\maketitle

\part{Introduction}

This document actually includes two topics. First, I write the main proposal for how to rationally select the $\alpha$ and $\beta$ parameters. Then I finish with a few quibbles with the original article that I did not think merited their own document.


\part{Proposal}
\section{Problem}
You introduce the $\alpha$ and $\beta$ parameters into your paper without giving a method to choose them. The performance of your algorithm is sensitive to those parameters, so this makes it difficult to apply in real life.
\section{Simulation Solution}
The key components of my solution to this problem are are: recognizing that your algorithm is a classifier and inventing a way of generating labeled training data for this classifier that in some way approximates the distribution of the input databases.
\subsection{Your algorithm as a classifier}
The goal of your algorithm is to classify pairs of features as either matching or non-matching. First you take the input databases and reduce them to two features based on your \func{MatchCount}, \func{PotentialMatchCount}, and \func{SubjectSize} metrics. I will call these features $A = \frac{\text{\func{MatchCount}}}{\text{\func{PotentialMatchCount}}}$ and $B = \frac{\ln \text{\func{MatchCount}}}{\ln \text{\func{SubjectSize}}}$. Next, you mark any pairs which have a non-zero \func{MatchCount} as matching (these are your "candidate matches." Then you mark those pairs that do not have the highest $A$ value for at least one of those parameters as non-matching. Finally you mark all pairs meeting $A < \alpha \wedge B < \beta$ as non-matching.\footnote{It might be interesting and useful to use other classifiers based on the same features, or on combinations of the three basic features for the last step. For example, you could train a decision tree based on vectors ($m$, $p$, $\frac{m}{p}$, $\frac{ln m}{ln s}$, $\frac{ln p}{ln s}$) where $m, p, s$ are \func{MatchCount}, \func{PotentialMatchCount}, and \func{SubjectSize} respectively. Note that I do not leave $s$ as a parameter on its own since the generated databases will have smaller $s$ values than the real world inputs.}

Because this is a classifier with parameters we would like to train it to select the best parameters. In your paper, you did the training by hand based on your own domain knowledge and selected the parameters. In this improvement, we will use a set of training data.\footnote{It is interesting to consider using an unsupervised method but I doubt that the classes cluster that well in the feature space we are considering here.}
\subsection{Generating the training data}
The training data will be property pairs that maximize the $A$ feature for one of their two properties in a particular random pair of databases. Conceptually, it will be generated by drawing a random database pair from a distribution that hopefully approximates the distribution from which the original input databases are drawn and then running the feature generation algorithm on that pair. Because of the nature of the problem, it is possible to generate the features without first generating the entire database, saving time and space. For clarity, however, I will explain the conceptual generation before presenting a more efficient realization of the procedure. 
\subsubsection{Conceptual training set generation}
The databases we are taking as input derive from different people looking at the world and selecting overlapping subsets of it to describe in overlapping ways. If you take one of the input databases and partition it into two subsets, you simulate this process on a smaller scale. This is the intuition behind my database generation method.

However, there are important things we do not know about the process which generated the real-world databases. We will need to integrate over them in our sampling methodology. Anything we can do to improve our knowledge of these will also improve the training set and thus, ultimately, the final classifier.

I have selected seven unknown characteristics which I hope will be sufficient to give a good approximation and embodied them in seven parameters: $p_1$, $n_1$, $n_2$, $n_b$, $r_1$, $r_2$, and $r_s$. These are, respectively, the probability of selecting the first input database as the source, the number of properties only in database 1, the number of properties only in database 2, the number of properties in both, the fraction of the tuples that are removed from database 1, the fraction that are removed from database 2, and the fraction of same-as tuples that are removed. These parameters have three constraints. First, $n_1 + n_2 + n_b \leq n$ where $n$ is the total number of properties in the original database. Second, neither database can be empty, so $n_1 \geq 1$, $n_2 \geq 1$, and $n_b \geq 0$. Finally, $0 \leq r_1 < 1$ and $0 \leq r_2 < 1$ and $0 \leq r_s < 1$, so you can't remove all the tuples.

To generate a database, you first randomly select an input database. Then, you randomly select 3 non-overlapping subsets of properties of sizes $n_1$, $n_2$, and $n_b$. The tuples for the first set of properties go into database 1. The tuples for the second set of properties go into database 2. And the tuples from the third set go into both databases. Same-as tuples are generated on the same objects that had direct and inferred same-as links to the input database not used as the source.

Next, $r_1 | db_1 |$ tuples are randomly selected and removed from database 1 and similarly for database 2.

Finally, select and remove $r_s$ of the same-as links.

To reduce the number of parameters, it may be good to let $r_1 = r_2$ and $n_1 + n_2 + n_b = n$. You could also use prior information that $\frac{n_1+n_b}{n_2+n_b}=\frac{|numPropertiesDB1|}{|numPropertiesDB2|}$ (so the generated databases have the same size ratio for number of properties.)

\subsubsection{Actual training set generation}
In concept, we are generating databases and then analyzing them. But since all we care about is the sets of training instances to come out of them, we can omit the actual database generation. I present here an algorithm that generates a training set\footnote{Depending on your memory constraints and IO costs, it may be effective to simultaneously generate several thousand training sets from several thousand simulated databases on each pass.} with a single pass through the source database.

Have the input database formatted as tables:
\begin{enumerate}
\item instance-id $\times$ has-same-as-link
\item subject-id $\times$ object-id $\times$ property-id
\end{enumerate}

Note that the characteristics of triples whose subject does not have a same-as link will only affect subject-count, so they could be stored in a third table and given special treatment for efficiency reasons. For clarity and ease of writing, I will not do that here.

Sort the database (and/or query) by (subject-id,object-id)

Now perform the following operation:

\begin{enumerate}
\item Decide whether each "same-as" tuple is included. 
      This information should be small enough to keep in RAM.
\item Partition the properties between being in one database, 
      in the other database, or in both and keep this assignment in RAM
\item Keep match counts and potential match counts in RAM 
      for each pair of properties.
\item For each subject-object pair (so-pair) in the original database
	\begin{enumerate}
	\item Read all the properties that contain that so-pair.  Since the 
	      query is sorted, this is just reading until the next so-pair.
	\item Each property generates 1 or 2 virtual tuples (1 tuple in 
	      the appropriate database if it is in only one database, 2 if 
	      it is in both)
	\item Determine whether the virtual tuples were deleted for each property. 
	\item Make a new local assignment of which property appears in 
	      which database depending on where the virtual 
	      tuples are. So if there are no virtual tuples, 
	      this property doesn't appear in either database. 
	      If there is only the first tuple, it is only in the first 
	      database. (This assignment will go away for the next so-pair.)
	\item For each possible ordered pair of properties $P_1$,$P_2$ where the first appears in database 1 and the second appears in database 2.
	      \begin{enumerate}
	      \item If the subject and object both have same-as links, 
	            increment \func{MatchCount}($P_1$, $P_2$)
	      \item If the subject has a same-as link, then increment \\ 
	      	    \func{PotentialMatchCount}($P_1$, $P_2$)
	      \end{enumerate}
	\end{enumerate}
\end{enumerate}

This yields a set of filled-in \func{MatchCount} and \func{PotentialMatchCount} values for all pairs of properties. We can ignore all pairs where \func{MatchCount} is 0.

Choose the strongest match (highest $A$) for each property in the first database and the strongest for each property in the second database. (If they are not the same, I would discard the lower $A$ match as spurious, but you don't mention that in your paper --- maybe it doesn't work well?) These property pairs along with their associated match counts, potential match counts, and database subject sizes are the input instances. We can label them: when $P_1 = P_2$ then we have a genuine match. When they are different we have a spurious match. 

\subsection{Training the classifier}
Now there are two approaches we could take, depending on whether we are using an off-the-shelf classifier or the classifier from the paper.

\subsubsection{Off-the-shelf classifier}
If we are using an off-the-shelf classifier, we accumulate all these instances over all generated databases. Then we train our classifier that using the features $\frac{\text{\func{MatchCount}}}{\text{\func{PotentialMatchCount}}}$ and $\frac{\ln \text{\func{MatchCount}}}{\ln \text{\func{SubjectSize}}}$. If we're feeling really bold, we could throw a couple of more features into the mix like $\func{MatchCount}$ itself or the ratio $\frac{\func{SubjectsInDB1}}{\func{SubjectsInDB2}}$. If the user has specified constraints like ``minimum precision" then we use cross-validation to choose the classifier that best fits the user's constraints. Because of the mismatch between the distribution from which the input databases are drawn and our simulated distribution, these will only be rough estimates. However, I expect that they will be conservative estimates due to the Monte Carlo integration over the unknown parameters.

\subsubsection{Classifier from the paper}
The second option is to roll our own training procedure and classifier. A simple version of this would be to create a grid of bins where one axis has a bin boundary half-way between each adjacent pair of $A$ values and the other axis between each adjacent pair of $B$ values. Each bin contains a $2 \times 2$ confusion matrix. For each instance, update all the bins according to how that instance would be classified with those values of $\alpha$ and $\beta$. Now choose the bin which best satisfies the user's constraints or which maximizes an evaluation measure (like AUC, for example).

\subsection{Improving the quality of the training set}
Because the quality of the training set is so important, it might be good to make use of more information in the data to improve the training set by inferring which parameters are most likely to yield the distributions of $A$ and $B$ seen in the original data, then using these parameters to generate the training set. This is an attempt to infer the hidden transformations of reality that generated the initial databases, assuming that our model has the right parameters to simulate it.

This can be accomplished in several ways. One way is to write a regressor that takes in the $A$ and $B$ distributions from a database and infers the parameters of the simulation that generated that database. Next take that regressor and run it on the distributions from the original input databases. Now, make a new set of simulated databases using the inferred parameter distributions to train the classifier that will actually match the properties on the input databases.

One could also do this using a classifier. Divide each of the parameter ranges in half. Create a classifier to distinguish between databases from one range and the other based on their $A$ and $B$ histograms.\footnote{Or some other representation of the distributions} If the classifier performs significantly better than chance, find which set of parameter ranges the original would be classified as and repeat with that range. Finally, when the data is useless for distinguishing parameters use it instead to train the classifier for properties.

A soft version of the previous might also be useful. If you have a classifier that outputs probabilities of class membership, rather than replacing the ranges at each step, you can instead sample according to the predicted distribution and terminate when the predicted distribution no longer changes.

\subsection{Problems}
Simulated databases are smaller than actual databases - is the denominator correct on $F'$?

Doesn't account for spurious same-as links

Is the model of the simulated databases good enough to give good results on real databases.
{\Huge Finish THIS}
\part{Quibbles}


\section{Assymetric Metric}

The \owl{owl:equivalentProperty} relation is symmetric. The $\func{PotentialMatchCount}$
function is defined to be asymmetric, that is, $\func{PotentialMatchCount}(P_{1},P_{2})\neq\func{PotentialMatchCount}(P_{2},P_{1})$.
The same goes for $\func{MatchCount}(P_{1},P_{2})$ as well. (Though $\func{MatchCount}$ has constraints that may make it symmetric despite the asymmetric definition, I'd need to think about it more to be sure.) Usually, in my experience, this difference in symmetry means that there is a better metric that uses
the information on the symmetry.

\section{$F'$ could be written more simply}

You define 

\begin{align*} 
&F'=\frac{{\ln(\func{MatchCount}(P_{1},P_{2}))}}{\ln(\func{SubjectSize}(D_{1},D_{2}))}\geq\beta \\
 & \text{where } \\
 & \func{SubjectSize}(D_{1},D_{2})=\operatorname{min}(|S:\exists SPO\operatorname{in}D_{1}|,|S:\exists SPO\operatorname{in}D_{2}|) 
\end{align*} 

I think it is much clearer to write this as:

\begin{align*} 
&F'=\func{MatchCount}(P_{1},P_{2})) \geq S^\beta \\
 & \text{where } \\
 & S=\operatorname{min}(|S:\exists SPO\operatorname{in}D_{1}|,|S:\exists SPO\operatorname{in}D_{2}|) 
\end{align*} 


\end{document}
