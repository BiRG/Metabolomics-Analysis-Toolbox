%% LyX 2.0.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage{amsmath}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\newcommand{\owl}[1]{\textit{#1}} % Format owl properties
\newcommand{\func}[1]{\texttt{#1}} % Format function names

\makeatother

\usepackage{babel}
\begin{document}

\title{Proposal for Removing Twiddle Parameters in LOD Property Alignment
Algorithm}


\author{Eric Moyer}


\date{26 August 2012}

\maketitle

\section{Introduction}

This document actually includes two topics. First, I write the main proposal for how to rationally select the $\alpha$ and $\beta$ parameters. Then I finish with a few quibbles with the original article that I did not think merited their own document.


\section{Proposal}
\subsection{Problem}
You introduce the $\alpha$ and $\beta$ parameters into your paper without giving a method to choose them. The performance of your algorithm is sensitive to those parameters, so this makes it difficult to apply in real life.
\subsection{Simulation Solution}
We can select the $\alpha$ and $\beta$ parameters for a given pair of databases by using the information in the original databases to create simulated database pairs which mimic properties of the original databases. Then we can select the parameters that satisfy the user constraints on precision and recall. One could also use a similar method, but instead select the best classifier based on some single measure of classifier performance like AUC.
\subsubsection{Main loop}
The main concept is
\begin{enumerate}
  \item Get user constraints on solution. Desired minimum precision or     
        desired minimum recall. 
  \item Generate lots of random databases with known correspondences - 
        the parameters in the database generation step are generated randomly.
  \item Calculate the precision and recall for all possible 
        $(\alpha,\beta)$ pairs on those databases.
  \item Take a satisficing approach to satisfying the constraints. If minimum
        precision was specified, find the pairs 
        with that precision and above and select the pair with the maximum
        recall. If no pair meets the minimum precision, select the pair with the
        maximum precision.
        
        If the minimum recall was specified, follow the same procedure
        interchanging precision and recall.
\end{enumerate}

Because generating the databases is likely to be the resource intensive step, it is probably better to keep a grid of $\alpha$ and $\beta$ ranges where the confusion matrix is constant for a particular database and merge those all into a final grid. 

The counts for true/false positive can probably be calculated during the database "generation" phase without any need to actually instantiate the new database.

\subsubsection{Combining Confusion Matrices}

To combine two confusion matrices where the $\alpha$ and $\beta$ intervals overlap, break them into rectangles that cover the overlap and the areas with no overlap. The areas with no overlap have the same confusion matrices. The areas of overlap have matrices that are the sum of the entries in the original areas.

Note: this treats each potential match in each simulated database as equivalent. We could also just store the list of calculated P/R values in the interval and then make assumptions about their distribution to decide whether one $\alpha$, $\beta$ interval is better than another.

\subsubsection{Generating a simulated database pair from an input database}
Before I start note that this is a conceptual way to generate the simulated database. It seems likely that with the counts from the original database and an assignment of properties to the sub-databases, one can quickly calculate the appropriate confusion matrix values for all combinations of parameters.

To generate a simulated database pair, you start with your input database and 5 parameters: $n_1$, $n_2$, $n_b$, $r_1$, and $r_2$. These are, respectively, the number of properties only in database 1, the number of properties only in database 2, the number of properties in both, the fraction of the tuples that are removed from database 1, and the fraction that are removed from database 2. These parameters have three constraints. First, $n_1 + n_2 + n_b \leq n$ where $n$ is the total number of properties in the original database. Second, neither database can be empty, so $n_1 \geq 1$, $n_2 \geq 1$, and $n_b \geq 0$. Finally, $0 \leq r < 1$ and $0 \leq r < 1$, so you can't remove all the tuples.

Then, you randomly select 3 non-overlapping subsets of properties of sizes $n_1$, $n_2$, and $n_b$. The tuples for the first set of properties go into database 1. The tuples for the second set of properties go into database 2. And the tuples from the third set go into both databases. Same-as tuples are generated on the same objects that had direct and inferred same-as links to the database not used as input.

Next, $r_1 | db_1 |$ tuples are randomly selected and removed from database 1 and similarly for database 2.

To reduce the number of parameters, it may be good to let $r_1 = r_2$ and $n_1 + n_2 + n_b = n$. You could also use prior information that $\frac{n_1+n_b}{n_2+n_b}=\frac{|db_1|}{|db_2|}$ (so the generated databases have the same size ratio for number of properties.)

It might be a good idea to have a separate same-as tuple removal rate. I am not sure it is a good assumption that the same-as tuples will be missing at the same rate as the original tuples.
\subsubsection{Create Classifier}

Have the input database formatted as tables:
\begin{enumerate}
\item instance-id $\times$ has-same-as-link
\item subject-id $\times$ object-id $\times$ property-id
\end{enumerate}

Sort the database (and/or query) by (subject-id,object-id)

Now perform the following operation:

\begin{enumerate}
\item Decide whether each "same-as" tuple is included. 
      This information should be small enough to keep in RAM.
\item Partition the properties between in 1 database, 
      in the other database, or in both and keep this assignment in RAM
\item Keep match counts and potential match counts in RAM 
      for each pair of properties.
\item For each subject-object pair (so-pair) in the original database
	\begin{enumerate}
	\item Read all the properties that contain that so-pair (which is just 
	      read until the next so-pair, since the query is sorted)
	\item Each property generates 1 or 2 virtual tuples (1 tuple in 
	      the appropriate database if it is in only one database, 2 if 
	      it is in both)
	\item Determine whether the virtual tuples were deleted for each property. 
	\item Make a new local assignment of which property appears in 
	      which database depending on where the virtual 
	      tuples are. So if there are no virtual tuples, 
	      this property doesn't appear in either database. 
	      If there is only the first tuple, it is only in the first 
	      database. (This assignment will go away for the next so-pair.)
	\item For each possible ordered pair of properties $P_1$,$P_2$ where the first appears in database 1 and the second appears in database 2.
	      \begin{enumerate}
	      \item If the subject and object both have same-as links 
	            increment \func{MatchCount}($P_1$, $P_2$)
	      \item If the subject has a same-as link, then increment \\ 
	      	    \func{PotentialMatchCount}($P_1$, $P_2$)
	      \end{enumerate}
	\end{enumerate}
\end{enumerate}

This yields a set of filled-in \func{MatchCount} and \func{PotentialMatchCount} values for all pairs of properties. We can ignore all pairs where \func{MatchCount} is 0.

Choose the strongest match (highest $\alpha$) for each property in the first
database and the strongest for each property in the second database. (If they are
not the same, I would discard them as spurious, but you don't mention that in
your paper --- maybe it doesn't work well?) These property pairs along with their
associated match counts, potential match counts, and database subject sizes are
the input instances. We can label them: when $P_1 = P_2$ then we have a genuine
match. When they are different we have a spurious match. 

Now there are three approaches we could take. The easiest is to just accumulate all these instances over all generated databases. Then use this as a training set to train a classifier that uses the features $\frac{\text{\func{MatchCount}}}{\text{\func{PotentialMatchCount}}}$ and $\frac{\ln \text{\func{MatchCount}}}{\ln \text{\func{SubjectSize}}}$. If we're feeling really bold, we could throw a couple of more features into the mix like $\func{MatchCount}$ itself or the ratio $\frac{\func{SubjectsInDB1}}{\func{SubjectsInDB2}}$. Then we use cross-validation to choose the classifier that best fits the user's constraints.

The second option is to roll our own training procedure and classifier. A simple version of this would be to create a grid of bins where one axis has a bin boundary half-way between each adjacent pair of $\alpha$ values and the other axis between each adjacent pair of $\beta$ values. Each bin contains a $2 \times 2$ confusion matrix. For each instance, update all the bins according to how that instance would be classified with those values of $\alpha$ and $\beta$. Now choose the bin which best satisfies the user's constraints.

The final option is to, instead of classifying directly, try to infer the characteristics of the transformations of reality that created the original input databases. Run this on the original input and then simulate again using the narrowed estimate of the parameter space from which the original databases could have been generated. In other words, write a regressor that takes in the $\alpha$ and $\beta$ distributions from a database and infers the parameters of the simulation that generated that database. Next take that regressor and run it on the distributions from the original input databases. Now, make a new set of simulated databases using the inferred parameter distributions. Finally, use one of the first two options to create the classifier that will actually match the properties.

\subsubsection{Problems}
Simulated databases are smaller than actual databases - is the denominator correct on $F'$?

Doesn't account for spurious same-as links
{\Huge Finish THIS}
\section{Quibbles}


\subsection{Assymetric Metric}

The \owl{owl:equivalentProperty} relation is symmetric. The $\func{PotentialMatchCount}$
function is defined to be asymmetric, that is, $\func{PotentialMatchCount}(P_{1},P_{2})\neq\func{PotentialMatchCount}(P_{2},P_{1})$.
The same goes for $\func{MatchCount}(P_{1},P_{2})$ as well. (Though $\func{MatchCount}$ has constraints that may make it symmetric despite the asymmetric definition, I'd need to think about it more to be sure.) Usually, in my experience, this difference in symmetry means that there is a better metric that uses
the information on the symmetry.

\subsection{F' could be written more simply}

You define 

\begin{align*} 
&F'=\frac{{\ln(\func{MatchCount}(P_{1},P_{2}))}}{\ln(\func{SubjectSize}(D_{1},D_{2}))}\geq\beta \\
 & \text{where } \\
 & \func{SubjectSize}(D_{1},D_{2})=\operatorname{min}(|S:\exists SPO\operatorname{in}D_{1}|,|S:\exists SPO\operatorname{in}D_{2}|) 
\end{align*} 

I think it is much clearer to write this as:

\begin{align*} 
&F'=\func{MatchCount}(P_{1},P_{2})) \geq S^\beta \\
 & \text{where } \\
 & S=\operatorname{min}(|S:\exists SPO\operatorname{in}D_{1}|,|S:\exists SPO\operatorname{in}D_{2}|) 
\end{align*} 


\end{document}
